# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16_WxcdvPtlb5OfNKUGdijyfsTjsHcFdm
"""

import sys
import time
import csv
import zipfile
import collections
from IPython.display import display, HTML

# Increase the output display limit
display(HTML("<style>.container { max-width:100% !important; }</style>"))

SIZE = 40
ERROR = 10

def read_fasta(file_path):
    reads_info = []
    current_read = ""
    read_number = -1

    with open(file_path, 'r') as file:
        for line in file:
            line = line.strip()

            if line.startswith(">"):
                if current_read:
                    # Append information for the current read
                    reads_info.append({
                        'read': current_read,
                        'read_number': read_number,
                        'best_genome_match': 0,
                        'num_mismatches': 100
                    })
                    current_read = ""
                read_number += 1
            else:
                current_read += line

        # Append information for the last read
        if current_read:
            reads_info.append({
                'read': current_read,
                'read_number': read_number,
                'best_genome_match': 0,
                'num_mismatches': 100
            })

    return reads_info



def parse_reference_fasta(file_path):

        with open(file_path, 'r') as fasta_file:
            reference_genome = ''
            for line in fasta_file:
                if not line.startswith('>'):
                    reference_genome += line.strip()
        return reference_genome




def find_positions(reference):

    index_dict = {}

    #do the divide into thirds thing
    size = int(SIZE/3)

    for i in range(len(reference) - size + 1):
        k = reference[i:(i + size)]
        if k not in index_dict:
            index_dict[k] = [i]
        else:
            index_dict[k].append(i)

    return index_dict

#make reads smaller
def minimize(input_reads):
    kmers = []

    for read_info in input_reads:
        read = read_info['read']
        for i in range(len(read) - SIZE + 1):
            kmers.append(read[i:(i + SIZE)])

    return kmers

#find out how often they occur
def counter(reads):
    count_dict = {}

    for read in reads:
        if read not in count_dict:
            count_dict[read] = 1
        else:
            count_dict[read] += 1

    return count_dict

#give a list of possible locations for the reads
def get_index_list(k, index):
    # Set up variables
    index_list = []
    size = int(SIZE / 3)
    third_one = k[0:size]
    third_two = k[size:(2 * size)]
    third_three = k[(2 * size):]

    # Check each third for matches
    if third_one in index:
        index_list.extend(index[third_one])

    if third_two in index:
        second_matching_indices = [i - size for i in index[third_two] if i - size >= 0]
        index_list.extend(second_matching_indices)

    # Check if the third part matches perfectly
    if third_three in index:
        third_matching_indices = [i - (2 * size) for i in index[third_three] if i - (2 * size) >= 0]
        index_list.extend(third_matching_indices)

    return index_list


def find_matching_sections(kmer, genome_reference, try_indexes):
    matching_sections = []

    for index in try_indexes:
        section = genome_reference[index:(index + SIZE)]

        if len(section) < SIZE:
            continue

        # Ensure that the length of kmer and section does not exceed SIZE
        kmer = kmer[:min(len(kmer), len(section))]
        section = section[:min(len(kmer), len(section))]


        num_mismatches = sum(kmer[i] != section[i] for i in range(len(section)))

        if 0 < num_mismatches <= ERROR:
            matching_sections.append((num_mismatches))

    return matching_sections





# takes in a kmer and its matching sections and returns any SNPs
def input_matching_sections(kmer, matching_sections):
    # Maps a location to its SNP
    kmer_snps = {}

    for ref_section, start_index in matching_sections:
        # Initialize lists to store insertions and deletions
        insertions = []
        deletions = []

        for i in range(SIZE):
            if kmer[i] != ref_section[i]:
                if kmer[i] == '-' or ref_section[i] == '-':
                    # Found an insertion or deletion
                    if kmer[i] == '-':
                        deletions.append(start_index + i)
                    else:
                        insertions.append((start_index + i, ref_section[i]))

                else:
                    # Found a substitution
                    reference_allele = ref_section[i]
                    variant_allele = kmer[i]
                    location = start_index + i
                    snp = [reference_allele, variant_allele, location]
                    if location not in kmer_snps:
                        kmer_snps[location] = snp

        # Add insertions and deletions to the result
        for insertion in insertions:
            kmer_snps[insertion] = ['-', insertion[1], insertion[0]]

        for deletion in deletions:
            kmer_snps[deletion] = [deletion, '-', reference[start_index + i]]

    return kmer_snps


# takes in kmers and a reference index and returns the SNPs
def get_snps(kmers, reference_index, reference):
    # Maps an index to the SNP at that index
    snps = {}

    for kmer in kmers:
        # Get all the possible indices that the kmer could start at
        possible_indices = get_index_list(kmer, reference_index)

        # Remove duplicates from possible_indices
        possible_indices = list(dict.fromkeys(possible_indices))

        # Get matching sections in the reference genome
        # Each matching section is a (kmer, index) pair
        matching_sections = find_matching_sections(kmer, reference, possible_indices)

        # If there's no matching sections, there won't be any variations
        if len(matching_sections) == 0:
            continue

        # Find the variations and add them to our result
        curr_snps = input_matching_sections(kmer, matching_sections)

        # Add the variations to our result (avoiding adding multiple variations at the same location)
        for index in curr_snps:
            if index not in snps:
                snps[index] = curr_snps[index]

    # Return only the variations
    return list(snps.values())


def map_reads_to_genomes(num_genomes, reads_file, genomes_files, threshold=1000):
    genomes_to_include = [12, 31, 63, 67, 73, 84, 85, 91, 99]

    # Create a filtered list of genome indices
    filtered_genome_indices = [i for i in range(num_genomes) if i in genomes_to_include]

    # Read the reads with associated information
    input_reads = read_fasta(reads_file)
    genome_counts = {i: 0 for i in range(num_genomes)}



    # Loop through each genome in the filtered list
    for genome_idx in range(len(filtered_genome_indices)):
        # Read the current genome
        current_genome = parse_reference_fasta(genomes_files[genome_idx])

        # Find positions in the current genome
        reference_index = find_positions(current_genome)

        # Loop through each read
        for read_info in input_reads:
            read = read_info['read']
            read_number = read_info['read_number']
            current_mismatches = read_info['num_mismatches']
            current_best = read_info['best_genome_match']

            # Get possible indices in the current genome
            possible_indices = get_index_list(read, reference_index)
            possible_indices = list(dict.fromkeys(possible_indices))

            # Find matching sections in the current genome
            matching_sections = find_matching_sections(read, current_genome, possible_indices)

            # Check if there are matching sections
            if matching_sections:
                # Find the lowest number of mismatches
                lowest_mismatches = min(matching_sections)

                # Update the read_info if the lowest mismatches are lower than the current best
                if lowest_mismatches < current_mismatches and (genome_counts[genome_idx] >= threshold or current_best == 0):
                    read_info['num_mismatches'] = lowest_mismatches
                    read_info['best_genome_match'] = genomes_to_include[genome_idx]

                    genome_counts[genome_idx] += 1






    csv_file_path = "predictions.csv"
    with open (csv_file_path, "w", newline='') as csv_file:
        csv_writer = csv.writer(csv_file)

        for read_info in input_reads:
          read_number = read_info['read_number']
          best_genome_match = read_info['best_genome_match']
          to_output = f">read_{read_number} Genome_Number_{best_genome_match}"
          csv_writer.writerow([to_output])


    print(f"CSV file '{csv_file_path}' created successfully.")

    # Create a zip file containing the CSV file
    zip_file_path = "predictions.zip"
    with zipfile.ZipFile(zip_file_path, "w") as zip_file:
        zip_file.write(csv_file_path, "predictions.csv")

    print(f"Zip file '{zip_file_path}' created successfully.")






if __name__ == "__main__":


  num_genomes = 100  # Replace with the actual number of genomes
  reads_file = 'project1c_reads.fasta'  # Replace with the actual reads file
  genomes_files = ['project1c_genome_{}.fasta'.format(i) for i in range(num_genomes)]

  genomes_to_include = [12, 31, 63, 67, 73, 84, 85, 91, 99]

  # Remove the excluded genomes from the list
  genomes_files = [file for i, file in enumerate(genomes_files) if i in genomes_to_include]

  map_reads_to_genomes(num_genomes, reads_file, genomes_files)

