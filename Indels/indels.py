# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14cV5X2Zly6ark3B8BUKBMKtqf5FoIrwB
"""

import sys
import time
import csv
import zipfile
import numpy as np
from IPython.display import display, HTML

# Increase the output display limit
display(HTML("<style>.container { max-width:100% !important; }</style>"))

SIZE = 48
INDEL_THRESHOLD = 20
ERROR = 2

def read_fasta(file_path):
    reads = []
    current_read = ""

    with open(file_path, 'r') as file:
        for line in file:
            line = line.strip()

            if line.startswith(">"):
                if current_read:
                    reads.append(current_read)
                current_read = ""
            else:
                current_read += line

        #put this here to add last read- for some reason wasn't working
        if current_read:
            reads.append(current_read)

    return reads


def parse_reference_fasta(file_path):

        with open(file_path, 'r') as fasta_file:
            reference_genome = ''
            for line in fasta_file:
                if not line.startswith('>'):
                    reference_genome += line.strip()
        return reference_genome




def find_positions(reference):

    index_dict = {}

    #do the divide into thirds thing
    size = int(SIZE/3)

    for i in range(len(reference) - size + 1):
        k = reference[i:(i + size)]
        if k not in index_dict:
            index_dict[k] = [i]
        else:
            index_dict[k].append(i)

    return index_dict

#make reads smaller
def minimize(reads):
    kmers = []

    for read in reads:
        for i in range(len(read) - SIZE + 1):
            kmers.append(read[i:(i + SIZE)])

    return kmers

#find out how often they occur
def counter(reads):
    count_dict = {}

    for read in reads:
        if read not in count_dict:
            count_dict[read] = 1
        else:
            count_dict[read] = count_dict[read] + 1

    return count_dict

#give a list of possible locations for the reads
def get_index_list(k, index):
    # Set up variables
    index_list = []
    size = int(SIZE / 3)
    third_one = k[0:size]
    third_two = k[size:(2 * size)]
    third_three = k[(2 * size):]
    thirds_matching_count = 0


    # Check each third for matches
    if third_one in index:
        index_list.extend(index[third_one])
        thirds_matching_count += 1

    if third_two in index:
        second_matching_indices = [i - size for i in index[third_two] if i - size >= 0]
        index_list.extend(second_matching_indices)
        thirds_matching_count += 1

    # Check if the third part matches perfectly
    if third_three in index:
        third_matching_indices = [i - (2 * size) for i in index[third_three] if i - (2 * size) >= 0]
        index_list.extend(third_matching_indices)
        thirds_matching_count += 1



    return index_list


def find_matching_sections(kmer, genome_reference, try_indexes):
    matching_sections = []

    for index in try_indexes:
        section = genome_reference[index:(index+SIZE)]

        num_mismatches = sum(kmer[i] != section[i] for i in range(SIZE))

        if 0 < num_mismatches <= ERROR:
            matching_sections.append((section, index))

        if num_mismatches > INDEL_THRESHOLD:
            # Shift the alignment to the right by one index
            section = genome_reference[index + 1 : (index + 1 + SIZE)]
            # Check the number of mismatches again
            num_mismatches_shifted = sum(kmer[i] != section[i] for i in range(SIZE))

            if 0 < num_mismatches_shifted <= ERROR:
                matching_sections.append((section, index + 1))

            section_left_shifted = genome_reference[index : (index + SIZE)]
            num_mismatches_left_shifted = sum(kmer[i] != section_left_shifted[i] for i in range(SIZE))

            if 0 < num_mismatches_left_shifted <= ERROR:
                matching_sections.append((section_left_shifted, index))



    return matching_sections





# takes in a kmer and its matching sections and returns any SNPs
def input_matching_sections(kmer, matching_sections, reference_index, reference):
    kmer_snps = {}

    for ref_section, start_index in matching_sections:

        for i in range(SIZE):
            if kmer[i] != ref_section[i]:
                reference_allele = ref_section[i]
                variant_allele = kmer[i]
                location = start_index + i
                snp = [reference_allele, variant_allele, location]
                if location not in kmer_snps:
                    kmer_snps[location] = snp

        # Check for indels using the indel detection algorithm
        indel_detected = detect_indel(kmer, ref_section, reference_index, start_index, reference)

        if indel_detected:
            # Update the result with the detected indel
            kmer_snps.update(indel_detected)

    return kmer_snps

# takes in kmers and a reference index and returns the SNPs
def get_snps(kmers, reference_index, reference):

    snps = {}

    for kmer in kmers:
        # try all possible indicies
        possible_indices = get_index_list(kmer, reference_index)

        # Remove duplicates
        possible_indices = list(dict.fromkeys(possible_indices))


        matching_sections = find_matching_sections(kmer, reference, possible_indices)


        if len(matching_sections) == 0:
            continue

        # Find the variations and add them to our result
        curr_snps = input_matching_sections(kmer, matching_sections)

        # Add the variations to our result (avoiding adding multiple variations at the same location)
        for index in curr_snps:
            if index not in snps:
                snps[index] = curr_snps[index]

    # Return only the variations
    return list(snps.values())

def get_snps_with_indels(kmers, reference_index, reference):
    snps = {}

    for kmer in kmers:
        possible_indices = get_index_list(kmer, reference_index)
        possible_indices = list(dict.fromkeys(possible_indices))

        matching_sections = find_matching_sections(kmer, reference, possible_indices)

        if len(matching_sections) == 0:
            continue

        # Find the variations including indels
        curr_snps = input_matching_sections(kmer, matching_sections, reference_index, reference)

        for index in curr_snps:
            if index not in snps:
                snps[index] = curr_snps[index]

    return list(snps.values())




import numpy as np

def needleman_wunsch(x, y, match=1, mismatch=1, gap=1):
    nx = len(x)
    ny = len(y)
    # Optimal score at each possible pair of characters.
    F = np.zeros((nx + 1, ny + 1))
    F[:, 0] = np.linspace(0, -nx * gap, nx + 1)
    F[0, :] = np.linspace(0, -ny * gap, ny + 1)
    # Pointers to trace through an optimal alignment.
    P = np.zeros((nx + 1, ny + 1))
    P[:, 0] = 3
    P[0, :] = 4
    # Temporary scores.
    t = np.zeros(3)
    for i in range(nx):
        for j in range(ny):
            if x[i] == y[j]:
                t[0] = F[i, j] + match
            else:
                t[0] = F[i, j] - mismatch
            t[1] = F[i, j + 1] - gap
            t[2] = F[i + 1, j] - gap
            tmax = np.max(t)
            F[i + 1, j + 1] = tmax
            if t[0] == tmax:
                P[i + 1, j + 1] += 2
            if t[1] == tmax:
                P[i + 1, j + 1] += 3
            if t[2] == tmax:
                P[i + 1, j + 1] += 4
    # Trace through an optimal alignment.
    i = nx
    j = ny
    rx = []
    ry = []
    while i > 0 or j > 0:
        if P[i, j] in [2, 5, 6, 9]:
            rx.append(x[i - 1])
            ry.append(y[j - 1])
            i -= 1
            j -= 1
        elif P[i, j] in [3, 5, 7, 9]:
            rx.append(x[i - 1])
            ry.append('-')
            i -= 1
        elif P[i, j] in [4, 6, 7, 9]:
            rx.append('-')
            ry.append(y[j - 1])
            j -= 1
    # Reverse the strings.
    rx = ''.join(rx)[::-1]
    ry = ''.join(ry)[::-1]
    return rx, ry



def detect_indel(kmer, ref_section, reference_index, start_index, reference):

    # Your Needleman-Wunsch implementation that returns the best alignment
    alignment_genome, alignment_kmer = needleman_wunsch(ref_section, kmer)




    gap_count_genome = alignment_genome.count('-')
    gap_count_kmer = alignment_kmer.count('-')



    if gap_count_genome <= ERROR and gap_count_genome > 0:
        # Shift the read to the right and check for improved alignment
        # we have found an insertion
        insertion_position = alignment_genome.find('-')
        if insertion_position != 0 and insertion_position != 47:
          return {start_index + insertion_position: ['-', alignment_kmer[insertion_position], start_index + insertion_position]}


    if gap_count_kmer <= ERROR and gap_count_kmer > 0:
        # we have found a deletion
        deletion_position = alignment_kmer.find('-')
        return {start_index + deletion_position: [alignment_genome[deletion_position], '-', start_index + deletion_position]}

    else:
      return {}










def find_indel_position(matrix1, matrix2):
    # Find the position with the highest improvement in alignment score
    best_position = 0
    best_score_difference = 0

    for i in range(1, len(matrix1[0])):
        score_difference = matrix2[-1, i] - matrix1[-1, i]
        if score_difference > best_score_difference:
            best_score_difference = score_difference
            best_position = i

    return best_position


if __name__ == "__main__":





    reads_fn = 'project1b-u_with_error_paired_reads.fasta'
    reference_fn = 'project1b-u_reference_genome.fasta'

    input_reads = read_fasta(reads_fn)
    reference = parse_reference_fasta(reference_fn)

    reference_index = find_positions(reference)

    kmers = minimize(input_reads)


    kmer_to_frequency = counter(kmers)

    kmer_to_frequency = {kmer: freq for kmer, freq in kmer_to_frequency.items() if freq > 1}

    kmers = list(kmer_to_frequency.keys())

    snps = get_snps_with_indels(kmers, reference_index, reference)



# Sort the SNPs based on the numerical order of the location
    substitutions = [snp for snp in snps if snp[0] != '-' and snp[1] != '-']
    insertions = [snp for snp in snps if snp[0] == '-']
    deletions = [snp for snp in snps if snp[1] == '-']



# Sort each list separately
    substitutions_sorted = sorted(substitutions, key=lambda x: x[2])
    insertions_sorted = sorted(insertions, key=lambda x: x[2])
    deletions_sorted = sorted(deletions, key=lambda x: x[2])

# CSV file path
    csv_file_path = "predictions.csv"

# Write sorted SNPs to the CSV file
    with open(csv_file_path, "w", newline='') as csv_file:
      csv_writer = csv.writer(csv_file)
      csv_writer.writerow(["mutation"])

      for snp in substitutions_sorted:
        reference_allele = snp[0]
        variant_allele = snp[1]
        location = snp[2]
        mutation = f">S{location} {reference_allele} {variant_allele}"
        csv_writer.writerow([mutation])
        print([mutation])


      for snp in insertions_sorted:
        reference_allele = snp[0]
        variant_allele = snp[1]
        location = snp[2]
        mutation = f">I{location} {variant_allele}"
        csv_writer.writerow([mutation])
        print([mutation])



      for snp in deletions_sorted:
        reference_allele = snp[0]
        variant_allele = snp[1]
        location = snp[2]
        mutation = f">D{location} {reference_allele}"
        csv_writer.writerow([mutation])
        print([mutation])



# Zip file path
    zip_file_path = "predictions (25).zip"

# Create a zip file containing the CSV file
    with zipfile.ZipFile(zip_file_path, "w") as zip_file:
      zip_file.write(csv_file_path, "predictions.csv")

    print(f"Zip file '{zip_file_path}' created successfully.")
